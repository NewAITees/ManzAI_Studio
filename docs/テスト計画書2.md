# Manzai Studio テスト駆動開発計画書

ManzAI Studioプロジェクトにおける包括的なテスト駆動開発（TDD）計画書を作成します。このテスト計画は、各開発工程において適切なテストを先に書き、それを満たすコードを実装する方針で設計されています。

## 1. テスト駆動開発の基本方針

1. **RED-GREEN-REFACTOR**サイクルの徹底
   - 先にテストを書き（RED）
   - 最小限のコードで通過させ（GREEN）
   - コードをリファクタリングする（REFACTOR）

2. **テストカバレッジの目標**
   - バックエンドコード: 90%以上
   - フロントエンドコンポーネント: 80%以上
   - 統合テスト: 主要ユーザーフロー100%カバー

3. **テスト分類**
   - ユニットテスト: 個々の関数・メソッドの検証
   - 統合テスト: コンポーネント間の連携確認
   - システムテスト: エンドツーエンドの動作確認
   - パフォーマンステスト: 応答時間、リソース使用量の検証

## 2. バックエンド (Python/Flask) テスト計画

### 2.1 APIエンドポイントテスト

```python
# tests/api/test_health_endpoint.py
def test_health_endpoint_returns_200():
    """APIのヘルスエンドポイントが200を返すことを確認"""
    response = client.get('/api/health')
    assert response.status_code == 200
    assert response.json['status'] == 'healthy'

# tests/api/test_generate_endpoint.py
def test_generate_endpoint_with_valid_topic():
    """有効なトピックでAPIが正常に漫才を生成することを確認"""
    response = client.post('/api/generate', json={'topic': 'テスト漫才'})
    assert response.status_code == 200
    assert 'script' in response.json
    assert len(response.json['script']) > 0
    
def test_generate_endpoint_with_missing_topic():
    """トピックが不足した場合にAPIが400エラーを返すことを確認"""
    response = client.post('/api/generate', json={})
    assert response.status_code == 400
    assert 'error' in response.json
```

### 2.2 Ollamaサービス統合テスト

```python
# tests/services/test_ollama_service.py
def test_generate_manzai_script_returns_valid_structure():
    """Ollamaサービスが有効な漫才スクリプト構造を返すことを確認"""
    service = OllamaService()
    script = service.generate_manzai_script("猫")
    
    assert isinstance(script, list)
    assert len(script) > 0
    
    for line in script:
        assert 'role' in line
        assert 'text' in line
        assert line['role'] in ['tsukkomi', 'boke']

@patch('requests.post')
def test_ollama_service_handles_connection_error(mock_post):
    """Ollamaサービスが接続エラーを適切に処理することを確認"""
    mock_post.side_effect = requests.exceptions.ConnectionError()
    
    service = OllamaService()
    with pytest.raises(OllamaServiceError):
        service.generate_manzai_script("テスト")
```

### 2.3 VoiceVoxサービス統合テスト

```python
# tests/services/test_voicevox_service.py
def test_generate_voice_returns_audio_data():
    """VoiceVoxサービスが有効な音声データを返すことを確認"""
    service = VoiceVoxService()
    audio_data = service.generate_voice("こんにちは", speaker_id=1)
    
    assert audio_data is not None
    assert len(audio_data) > 0
    
def test_get_timing_data_returns_valid_structure():
    """VoiceVoxサービスが有効なタイミングデータを返すことを確認"""
    service = VoiceVoxService()
    timing = service.get_timing_data("こんにちは", speaker_id=1)
    
    assert 'accent_phrases' in timing
    assert len(timing['accent_phrases']) > 0
    
    for accent in timing['accent_phrases']:
        assert 'moras' in accent
        for mora in accent['moras']:
            assert 'text' in mora
            assert 'start_time' in mora
            assert 'end_time' in mora
```

### 2.4 音声ファイル管理テスト

```python
# tests/services/test_audio_manager.py
def test_save_audio_file():
    """音声ファイルが正しく保存されることを確認"""
    manager = AudioManager()
    audio_data = b'test audio data'
    file_path = manager.save_audio(audio_data, "test_audio")
    
    assert os.path.exists(file_path)
    with open(file_path, 'rb') as f:
        saved_data = f.read()
    assert saved_data == audio_data
    
def test_get_audio_file():
    """音声ファイルが正しく取得できることを確認"""
    manager = AudioManager()
    audio_data = b'test audio data'
    file_path = manager.save_audio(audio_data, "test_audio")
    
    retrieved_data = manager.get_audio("test_audio")
    assert retrieved_data == audio_data
```

## 3. フロントエンド (JavaScript/React) テスト計画

### 3.1 コンポーネントテスト

```javascript
// tests/components/InputForm.test.js
import { render, screen, fireEvent } from '@testing-library/react';
import InputForm from '../../components/InputForm';

test('renders input form with topic field', () => {
  render(<InputForm onSubmit={() => {}} />);
  const inputElement = screen.getByPlaceholderText(/トピックを入力/i);
  expect(inputElement).toBeInTheDocument();
});

test('calls onSubmit with topic when form is submitted', () => {
  const handleSubmit = jest.fn();
  render(<InputForm onSubmit={handleSubmit} />);
  
  const input = screen.getByPlaceholderText(/トピックを入力/i);
  fireEvent.change(input, { target: { value: 'テスト漫才' } });
  
  const submitButton = screen.getByRole('button', { name: /生成/i });
  fireEvent.click(submitButton);
  
  expect(handleSubmit).toHaveBeenCalledWith('テスト漫才');
});
```

### 3.2 Live2D統合テスト

```javascript
// tests/components/Live2DDisplay.test.js
import { render, act } from '@testing-library/react';
import Live2DDisplay from '../../components/Live2DDisplay';

// Live2Dモックの設定
jest.mock('../../utils/live2d', () => ({
  loadModel: jest.fn(),
  setParameter: jest.fn()
}));

test('loads Live2D model on component mount', async () => {
  const live2d = require('../../utils/live2d');
  
  await act(async () => {
    render(<Live2DDisplay modelPath="test-model.model3.json" />);
  });
  
  expect(live2d.loadModel).toHaveBeenCalledWith("test-model.model3.json");
});

test('updates mouth parameter when playing audio', async () => {
  const live2d = require('../../utils/live2d');
  
  await act(async () => {
    const { getByTestId } = render(
      <Live2DDisplay 
        modelPath="test-model.model3.json"
        isPlaying={true} 
        mouthOpenValue={0.8}
      />
    );
  });
  
  expect(live2d.setParameter).toHaveBeenCalledWith("ParamMouthOpenY", 0.8);
});
```

### 3.3 音声同期テスト

```javascript
// tests/utils/lipSync.test.js
import { calculateMouthOpenness } from '../../utils/lipSync';

test('calculates correct mouth openness at a given time', () => {
  const timingData = {
    accent_phrases: [{
      moras: [
        { text: "こ", start_time: 100, end_time: 150 },
        { text: "ん", start_time: 150, end_time: 200 }
      ]
    }]
  };
  
  // 口が開いている時間帯
  expect(calculateMouthOpenness(timingData, 120)).toBeGreaterThan(0);
  
  // モーラとモーラの間（小さな値になるはず）
  expect(calculateMouthOpenness(timingData, 150)).toBeLessThan(0.5);
  
  // どのモーラにも該当しない時間帯
  expect(calculateMouthOpenness(timingData, 300)).toBe(0);
});
```

### 3.4 APIクライアントテスト

```javascript
// tests/api/apiClient.test.js
import { generateManzai, getAudio } from '../../api/apiClient';
import fetchMock from 'jest-fetch-mock';

beforeEach(() => {
  fetchMock.resetMocks();
});

test('generateManzai sends correct request and parses response', async () => {
  const mockResponse = { 
    script: [
      { role: 'tsukkomi', text: 'こんにちは' },
      { role: 'boke', text: 'どうも！' }
    ],
    audio_data: [
      { role: 'tsukkomi', audio_path: '/audio/1.wav' },
      { role: 'boke', audio_path: '/audio/2.wav' }
    ]
  };
  
  fetchMock.mockResponseOnce(JSON.stringify(mockResponse));
  
  const result = await generateManzai('テスト');
  
  expect(fetchMock).toHaveBeenCalledWith('/api/generate', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ topic: 'テスト' })
  });
  
  expect(result).toEqual(mockResponse);
});
```

## 4. システム統合テスト計画

### 4.1 エンドツーエンドテスト

```python
# tests/e2e/test_manzai_generation.py
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def test_complete_manzai_generation_flow():
    """漫才生成の全体フローが正常に動作することを確認"""
    driver = webdriver.Chrome()
    try:
        # アプリケーションにアクセス
        driver.get("http://localhost:5173")
        
        # トピック入力
        topic_input = driver.find_element(By.ID, "topic-input")
        topic_input.send_keys("猫")
        
        # 生成ボタンクリック
        generate_button = driver.find_element(By.ID, "generate-button")
        generate_button.click()
        
        # 生成完了を待機（最大60秒）
        WebDriverWait(driver, 60).until(
            EC.presence_of_element_located((By.ID, "play-button"))
        )
        
        # 再生ボタンクリック
        play_button = driver.find_element(By.ID, "play-button")
        play_button.click()
        
        # 音声再生が開始されることを確認
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "audio[data-playing='true']"))
        )
        
        # キャラクターが表示されていることを確認
        character_element = driver.find_element(By.ID, "live2d-canvas")
        assert character_element.is_displayed()
        
    finally:
        driver.quit()
```

### 4.2 キャラクター表示ウィンドウテスト

```python
# tests/e2e/test_display_window.py
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

def test_display_window_opens_correctly():
    """表示ウィンドウが正しく開くことを確認"""
    main_driver = webdriver.Chrome()
    try:
        # メインウィンドウにアクセス
        main_driver.get("http://localhost:5173")
        
        # 表示ウィンドウを開くボタンをクリック
        display_button = main_driver.find_element(By.ID, "open-display-button")
        display_button.click()
        
        # 新しいウィンドウが開くのを待機
        time.sleep(2)
        
        # すべてのウィンドウハンドルを取得
        window_handles = main_driver.window_handles
        assert len(window_handles) > 1
        
        # 表示ウィンドウに切り替え
        display_window = main_driver.window_handles[1]
        main_driver.switch_to.window(display_window)
        
        # 正しいURLが表示されていることを確認
        assert "display" in main_driver.current_url
        
        # UIコントロール要素が非表示であることを確認
        control_elements = main_driver.find_elements(By.CSS_SELECTOR, "button, input, select")
        assert len(control_elements) == 0
        
        # キャラクターが表示されていることを確認
        character_element = main_driver.find_element(By.ID, "live2d-canvas")
        assert character_element.is_displayed()
        
    finally:
        main_driver.quit()
```

## 5. 環境・パフォーマンステスト計画

### 5.1 メモリリーク検出テスト

```python
# tests/performance/test_memory_usage.py
import os
import psutil
import time
import requests

def test_no_memory_leak_during_repeated_generation():
    """繰り返し生成を行っても著しいメモリ増加がないことを確認"""
    process = psutil.Process(os.getpid())
    
    # 初期メモリ使用量を測定
    initial_memory = process.memory_info().rss / 1024 / 1024  # MB
    
    # 10回の生成を実行
    for i in range(10):
        response = requests.post('http://localhost:5000/api/generate', 
                                 json={'topic': f'テスト{i}'})
        assert response.status_code == 200
        time.sleep(1)  # 各リクエスト間に短い待機
    
    # 最終メモリ使用量を測定
    final_memory = process.memory_info().rss / 1024 / 1024  # MB
    
    # メモリ増加が20%未満であることを確認
    assert final_memory < initial_memory * 1.2
```

### 5.2 応答時間テスト

```python
# tests/performance/test_response_time.py
import time
import requests
import statistics

def test_api_response_time_within_limits():
    """APIレスポンス時間が許容範囲内であることを確認"""
    response_times = []
    
    # 5回の測定を実行
    for i in range(5):
        start_time = time.time()
        response = requests.post('http://localhost:5000/api/generate', 
                                 json={'topic': f'テスト{i}'})
        end_time = time.time()
        
        assert response.status_code == 200
        response_times.append(end_time - start_time)
    
    # 平均応答時間が10秒未満であることを確認
    avg_response_time = statistics.mean(response_times)
    assert avg_response_time < 10.0
    
    # 最大応答時間が15秒未満であることを確認
    max_response_time = max(response_times)
    assert max_response_time < 15.0
```

## 6. Visual/音声品質テスト計画

### 6.1 キャラクターレンダリングテスト

```python
# tests/visual/test_character_rendering.py
import cv2
import numpy as np
from selenium import webdriver
from selenium.webdriver.common.by import By
from PIL import Image
import io

def test_character_is_properly_rendered():
    """キャラクターが正しくレンダリングされることを確認"""
    driver = webdriver.Chrome()
    try:
        # 表示ページにアクセス
        driver.get("http://localhost:5173/display")
        
        # スクリーンショットを取得
        screenshot = driver.get_screenshot_as_png()
        img = Image.open(io.BytesIO(screenshot))
        img_array = np.array(img)
        
        # OpenCVでBGR形式に変換
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        
        # 緑色検出（クロマキー背景の確認）
        img_hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        green_lower = np.array([40, 100, 100])
        green_upper = np.array([80, 255, 255])
        green_mask = cv2.inRange(img_hsv, green_lower, green_upper)
        
        # 緑色のピクセルが全体の80%以上を占めることを確認（背景が緑色）
        green_percentage = (np.sum(green_mask > 0) / (green_mask.shape[0] * green_mask.shape[1])) * 100
        assert green_percentage > 80
        
        # キャラクターが描画されていることを確認（緑色以外の領域）
        character_pixels = np.sum(green_mask == 0)
        assert character_pixels > 1000
        
    finally:
        driver.quit()
```

### 6.2 音声品質テスト

```python
# tests/audio/test_voice_quality.py
import numpy as np
import librosa
import requests
import tempfile
import os

def test_audio_quality():
    """生成された音声の品質を検証"""
    # 漫才を生成
    response = requests.post('http://localhost:5000/api/generate', 
                             json={'topic': '音声テスト'})
    data = response.json()
    
    for audio_item in data['audio_data']:
        # 音声ファイルをダウンロード
        audio_url = f"http://localhost:5000/api/audio/{os.path.basename(audio_item['audio_path'])}"
        audio_response = requests.get(audio_url)
        
        # 一時ファイルに保存
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp:
            temp.write(audio_response.content)
            temp_path = temp.name
        
        try:
            # librosaで音声を分析
            y, sr = librosa.load(temp_path, sr=None)
            
            # 1. 音声が空でないことを確認
            assert len(y) > 0
            
            # 2. サンプリングレート確認
            assert sr >= 16000
            
            # 3. 無音区間が少ないことを確認
            non_silent = librosa.effects.split(y, top_db=20)
            non_silent_duration = sum(end - start for start, end in non_silent) / sr
            total_duration = len(y) / sr
            non_silent_ratio = non_silent_duration / total_duration
            assert non_silent_ratio > 0.5
            
            # 4. 音量レベルの確認
            rms = librosa.feature.rms(y=y)[0]
            assert np.mean(rms) > 0.01
            
        finally:
            # 一時ファイルを削除
            os.unlink(temp_path)
```

### 6.3 リップシンク同期テスト

```python
# tests/audio/test_lip_sync.py
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import numpy as np
from PIL import Image
import io

def test_lip_sync_accuracy():
    """リップシンク（口の動きと音声の同期）の精度を検証"""
    driver = webdriver.Chrome()
    try:
        # アプリにアクセス
        driver.get("http://localhost:5173")
        
        # トピック入力と漫才生成
        topic_input = driver.find_element(By.ID, "topic-input")
        topic_input.send_keys("リップシンクテスト")
        
        generate_button = driver.find_element(By.ID, "generate-button")
        generate_button.click()
        
        # 生成完了を待機
        WebDriverWait(driver, 60).until(
            EC.presence_of_element_located((By.ID, "play-button"))
        )
        
        # 再生ボタンをクリック
        play_button = driver.find_element(By.ID, "play-button")
        play_button.click()
        
        # フレームを一定間隔で取得
        frames = []
        audio_events = []
        
        # JavaScriptを実行して音声イベントを記録
        driver.execute_script("""
            window.audioEvents = [];
            var audio = document.querySelector('audio');
            if (audio) {
                audio.addEventListener('timeupdate', function() {
                    window.audioEvents.push({
                        time: performance.now(),
                        currentTime: audio.currentTime
                    });
                });
            }
        """)
        
        # 5秒間、0.1秒ごとにスクリーンショットを取得
        start_time = time.time()
        while time.time() - start_time < 5:
            screenshot = driver.get_screenshot_as_png()
            current_time = time.time() - start_time
            frames.append({
                'time': current_time,
                'image': screenshot
            })
            time.sleep(0.1)
        
        # 音声イベントを取得
        audio_events = driver.execute_script("return window.audioEvents;")
        
        # 口の開閉状態を分析
        mouth_states = []
        for frame in frames:
            img = Image.open(io.BytesIO(frame['image']))
            # 口周辺の領域を切り取り（座標は実際のモデルに合わせて調整）
            mouth_region = img.crop((400, 200, 600, 300))
            # グレースケールに変換して平均明度を計算（簡易的な口の開閉検知）
            mouth_region_gray = mouth_region.convert('L')
            mouth_openness = np.mean(np.array(mouth_region_gray))
            mouth_states.append({
                'time': frame['time'],
                'openness': mouth_openness
            })
        
        # 口の動きと音声イベントの相関を分析
        changes = detect_mouth_state_changes(mouth_states)
        score = calculate_sync_score(changes, audio_events)
        
        # 70%以上の同期率を期待
        assert score >= 0.7
        
    finally:
        driver.quit()

def detect_mouth_state_changes(mouth_states):
    """口の状態変化を検出"""
    changes = []
    threshold = 10  # 明度変化の閾値
    
    for i in range(1, len(mouth_states)):
        prev = mouth_states[i-1]['openness']
        curr = mouth_states[i]['openness']
        
        if abs(curr - prev) > threshold:
            changes.append({
                'time': mouth_states[i]['time'],
                'from': prev,
                'to': curr
            })
    
    return changes

def calculate_sync_score(mouth_changes, audio_events):
    """口の動きと音声イベントの同期スコアを計算"""
    if not mouth_changes or not audio_events:
        return 0
    
    matching_events = 0
    total_changes = len(mouth_changes)
    
    for change in mouth_changes:
        # 最も近い音声イベントを探す
        closest_event = min(audio_events, 
                            key=lambda e: abs(change['time'] - e['time']/1000))
        
        # 時間差が100ms以内なら一致とみなす
        if abs(change['time'] - closest_event['time']/1000) < 0.1:
            matching_events += 1
    
    return matching_events / total_changes
```

## 7. テスト自動化計画

### 7.1 CI/CD統合

```yaml
# .github/workflows/test.yml
name: Test

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      voicevox:
        image: voicevox/voicevox_engine:cpu-ubuntu20.04-latest
        ports:
          - 50021:50021
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
        
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install poetry
        poetry install
        
    - name: Run tests
      run: |
        poetry run pytest backend/tests/ --cov=backend
        
    - name: Set up Node.js
      uses: actions/setup-node@v2
      with:
        node-version: '18'
        
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm install
        
    - name: Run frontend tests
      run: |
        cd frontend
        npm test
```

### 7.2 テスト自動化ダッシュボード

```python
# tools/test_dashboard.py
import tkinter as tk
from tkinter import ttk
import subprocess
import threading
import queue
import json
import os

class TestDashboard:
    """テスト実行と結果表示のためのダッシュボード"""
    
    def __init__(self, root):
        self.root = root
        self.root.title("ManzAI Studio テストダッシュボード")
        self.root.geometry("800x600")
        
        self.output_queue = queue.Queue()
        
        self.create_widgets()
        self.update_output()
    
    def create_widgets(self):
        """ウィジェットの作成"""
        # テストスイート選択フレーム
        test_frame = ttk.LabelFrame(self.root, text="テストスイート")
        test_frame.pack(fill="x", padx=10, pady=10)
        
        # テストスイートのチェックボックス
        self.test_vars = {}
        test_suites = [
            "api", "services", "components", "e2e", "visual", "performance"
        ]
        
        for suite in test_suites:
            var = tk.BooleanVar(value=True)
            self.test_vars[suite] = var
            ttk.Checkbutton(
                test_frame, 
                text=f"{suite}テスト", 
                variable=var
            ).pack(anchor="w", padx=10, pady=5)
        
        # 実行ボタン
        ttk.Button(
            self.root, 
            text="テスト実行", 
            command=self.run_tests
        ).pack(pady=10)
        
        # 出力表示エリア
        output_frame = ttk.LabelFrame(self.root, text="テスト結果")
        output_frame.pack(fill="both", expand=True, padx=10, pady=10)
        
        self.output_text = tk.Text(output_frame, wrap="word", height=20)
        self.output_text.pack(fill="both", expand=True, padx=5, pady=5)
        
        # スクロールバー
        scrollbar = ttk.Scrollbar(self.output_text, command=self.output_text.yview)
        scrollbar.pack(side="right", fill="y")
        self.output_text.config(yscrollcommand=scrollbar.set)
    
    def run_tests(self):
        """選択されたテストを実行"""
        # 選択されたテストスイートを取得
        selected_suites = [
            suite for suite, var in self.test_vars.items() 
            if var.get()
        ]
        
        if not selected_suites:
            self.output_queue.put("エラー: テストスイートが選択されていません。")
            return
        
        # 出力をクリア
        self.output_text.delete(1.0, tk.END)
        self.output_queue.put("テスト実行を開始します...\n")
        
        # 別スレッドでテストを実行
        threading.Thread(
            target=self._run_tests_thread,
            args=(selected_suites,),
            daemon=True
        ).start()
    
テスト自動化ダッシュボードのコードが途中で切れているようです。続きを記載します。

```python
    def _run_tests_thread(self, selected_suites):
        """テスト実行スレッド"""
        try:
            # バックエンドテスト
            if any(suite in selected_suites for suite in ["api", "services", "e2e"]):
                self.output_queue.put("バックエンドテストを実行中...\n")
                
                # pytest実行
                cmd = ["poetry", "run", "pytest"]
                
                # 選択されたスイートに基づいてパスを指定
                test_paths = []
                if "api" in selected_suites:
                    test_paths.append("backend/tests/api/")
                if "services" in selected_suites:
                    test_paths.append("backend/tests/services/")
                if "e2e" in selected_suites:
                    test_paths.append("backend/tests/e2e/")
                
                cmd.extend(test_paths)
                cmd.append("--verbose")
                
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True
                )
                
                # 出力を逐次表示
                for line in process.stdout:
                    self.output_queue.put(line)
                
                process.wait()
                
                if process.returncode == 0:
                    self.output_queue.put("\nバックエンドテストが成功しました。\n")
                else:
                    self.output_queue.put("\nバックエンドテストが失敗しました。\n")
            
            # フロントエンドテスト
            if any(suite in selected_suites for suite in ["components"]):
                self.output_queue.put("フロントエンドテストを実行中...\n")
                
                # npm testの実行
                process = subprocess.Popen(
                    ["npm", "test", "--", "--watchAll=false"],
                    cwd="frontend",
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True
                )
                
                # 出力を逐次表示
                for line in process.stdout:
                    self.output_queue.put(line)
                
                process.wait()
                
                if process.returncode == 0:
                    self.output_queue.put("\nフロントエンドテストが成功しました。\n")
                else:
                    self.output_queue.put("\nフロントエンドテストが失敗しました。\n")
            
            # ビジュアルテスト
            if "visual" in selected_suites:
                self.output_queue.put("ビジュアルテストを実行中...\n")
                
                process = subprocess.Popen(
                    ["poetry", "run", "pytest", "backend/tests/visual/", "--verbose"],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True
                )
                
                for line in process.stdout:
                    self.output_queue.put(line)
                
                process.wait()
                
                if process.returncode == 0:
                    self.output_queue.put("\nビジュアルテストが成功しました。\n")
                else:
                    self.output_queue.put("\nビジュアルテストが失敗しました。\n")
            
            # パフォーマンステスト
            if "performance" in selected_suites:
                self.output_queue.put("パフォーマンステストを実行中...\n")
                
                process = subprocess.Popen(
                    ["poetry", "run", "pytest", "backend/tests/performance/", "--verbose"],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True
                )
                
                for line in process.stdout:
                    self.output_queue.put(line)
                
                process.wait()
                
                if process.returncode == 0:
                    self.output_queue.put("\nパフォーマンステストが成功しました。\n")
                else:
                    self.output_queue.put("\nパフォーマンステストが失敗しました。\n")
            
            self.output_queue.put("\nすべてのテストが完了しました。")
            
        except Exception as e:
            self.output_queue.put(f"エラーが発生しました: {str(e)}")
    
    def update_output(self):
        """出力キューからテキストを取得して表示"""
        try:
            while True:
                line = self.output_queue.get_nowait()
                self.output_text.insert(tk.END, line)
                self.output_text.see(tk.END)
                self.output_queue.task_done()
        except queue.Empty:
            pass
        finally:
            # 定期的に更新
            self.root.after(100, self.update_output)

if __name__ == "__main__":
    root = tk.Tk()
    app = TestDashboard(root)
    root.mainloop()
```

## 8. 実装フェーズ計画

テスト駆動開発に基づく実装フェーズの計画を以下に示します。各フェーズでは、先にテストを書き、それを満たすコードを実装します。

### フェーズ1: コアサービス実装 (1週目)

1. **OllamaサービスとAPI実装**
   - テスト: `test_ollama_service.py`と`test_generate_endpoint.py`を実装
   - 実装: `OllamaService`クラスと`/api/generate`エンドポイントを実装

2. **VoiceVoxサービス実装**
   - テスト: `test_voicevox_service.py`を実装
   - 実装: `VoiceVoxService`クラスを実装

3. **音声ファイル管理機能実装**
   - テスト: `test_audio_manager.py`を実装
   - 実装: `AudioManager`クラスと`/api/audio/<id>`エンドポイントを実装

### フェーズ2: フロントエンド基本実装 (2週目)

1. **入力フォームとAPI通信実装**
   - テスト: `InputForm.test.js`と`apiClient.test.js`を実装
   - 実装: 入力フォームコンポーネントとAPIクライアント関数を実装

2. **Live2D表示コンポーネント実装**
   - テスト: `Live2DDisplay.test.js`を実装
   - 実装: Live2Dライブラリを組み込んだ表示コンポーネントを実装

3. **音声再生コンポーネント実装**
   - テスト: `AudioPlayer.test.js`を実装
   - 実装: 音声再生とコントロール機能を実装

### フェーズ3: リップシンク機能実装 (3週目)

1. **モーラタイミング分析機能実装**
   - テスト: `test_get_timing_data`を実装
   - 実装: VoiceVoxからタイミングデータを取得する機能を実装

2. **リップシンク計算実装**
   - テスト: `lipSync.test.js`を実装
   - 実装: タイミングデータに基づく口の開閉度を計算する関数を実装

3. **同期再生機能実装**
   - テスト: `test_lip_sync_accuracy`を実装
   - 実装: 音声再生と口の動きを同期させるメカニズムを実装

### フェーズ4: デュアルウィンドウ機能実装 (4週目)

1. **表示ウィンドウ機能実装**
   - テスト: `test_display_window.py`を実装
   - 実装: 新しいウィンドウで表示する機能を実装

2. **クロマキー背景実装**
   - テスト: `test_character_rendering.py`を実装
   - 実装: 緑色背景と透明化処理を実装

3. **ウィンドウ間同期機能実装**
   - テスト: `test_window_synchronization.py`を実装
   - 実装: 2つのウィンドウ間でのステート同期機能を実装

### フェーズ5: 統合とリファクタリング (5週目)

1. **エンドツーエンドフロー完成**
   - テスト: `test_complete_manzai_generation_flow.py`を実装
   - 実装: 全コンポーネントを統合して完全なフローを実装

2. **パフォーマンス最適化**
   - テスト: `test_memory_usage.py`と`test_response_time.py`を実装
   - 実装: ボトルネックを特定して最適化を実施

3. **UI/UX改善**
   - テスト: ユーザビリティテストの実施
   - 実装: フィードバックに基づくUI/UX改善

## 9. テスト実行スケジュール

1. **毎日の開発テスト**
   - 変更に関連するユニットテストを実行
   - コードレビュー前に関連機能の統合テストを実行

2. **PR(Pull Request)ごとのテスト**
   - 自動CIで全ユニットテストとコア統合テストを実行
   - コードカバレッジレポートを生成

3. **機能完成時のテスト**
   - 完成した機能について全統合テストを実行
   - パフォーマンステストを実行

4. **リリース前テスト**
   - すべてのテストスイートを実行
   - エンドツーエンドテストを実行
   - 手動テストによる最終検証

## 10. テスト成功基準

1. **全テストが成功**: すべてのユニットテスト、統合テスト、システムテストが成功
2. **コードカバレッジ**: バックエンド90%以上、フロントエンド80%以上
3. **パフォーマンス目標達成**: 応答時間が規定値以内（初回生成15秒以内、音声同期100ms以内）
4. **品質基準達成**: 音声品質と視覚品質のテストが成功

## 結論

本テスト計画は、ManzAI Studioの開発をテスト駆動開発（TDD）アプローチで進める包括的なガイドラインです。適切なテストを先に書き、それに基づいてコードを実装することで、高品質なソフトウェアの開発が可能になります。各フェーズで設定されたテストを通過することで、確実に機能要件を満たし、安定した製品を提供できます。

テストはユニットレベルから統合レベル、さらにシステム全体にわたって設計されており、APIエンドポイント、バックエンドサービス、フロントエンドコンポーネント、そして視覚・音声品質に至るまで、アプリケーションの全側面をカバーしています。これにより、開発中の問題を早期に発見し、修正することが可能になります。